{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 請手刻gradient descent實作logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### read X_train and Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# 106 columns\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "with open('data/X_train.csv', 'rt', encoding='big5') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    row1 = next(reader) # skip headings\n",
    "    for idx, row in enumerate(reader):\n",
    "        X_train.append([float(i) for i in row] + [1.0])\n",
    "        \n",
    "        '''continuous_idxs = []\n",
    "        for idx2, num in enumerate(row):\n",
    "            if float(num) > 1:\n",
    "                continuous_idxs.append(idx2)\n",
    "        print(continuous_idxs)'''\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "for col in [0,1,3,4,5]: # [0,1,3,4,5] only, shouldn't normalize one-hot columns, will be affected by data\n",
    "    if np.std(X_train[:,col]) != 0:\n",
    "        X_train[:,col] = np.divide((X_train[:,col] - np.average(X_train[:,col])), np.std(X_train[:,col]))\n",
    "# [[data1]\n",
    "#  [data2]]\n",
    "\n",
    "with open('data/Y_train.csv', 'rt', encoding='big5') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    row1 = next(reader) # skip headings\n",
    "    for idx, row in enumerate(reader):\n",
    "        Y_train.append(float(row[0]))\n",
    "\n",
    "Y_train = np.array(Y_train)\n",
    "# [y1 y2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n",
      "[ 0.98373415  0.18819463  1.         -0.14592048 -0.21665953  0.36951938\n",
      "  0.          0.          0.          0.          0.          1.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          1.\n",
      "  0.          0.          0.          0.          0.          0.          1.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  1.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          1.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          1.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          1.          0.\n",
      "  0.          0.          1.        ]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train[0]))\n",
    "# print(X_train[0:2])\n",
    "# print(X_train[0:3])\n",
    "print(X_train[7])\n",
    "print(Y_train[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### define sigmoid function for numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x): # this sigmoid works with numpy arrays\n",
    "    res =  1.0 / (1 + np.exp(-x))\n",
    "    return np.clip(res, 0.00000000000001, 0.99999999999999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### start training, split X_train into X_real_train and X_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### currently not caring about validaton set, directly train with X_real_train = X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### accuracy stuck at 0.82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  0.  1.  1.  1.  0.  1.  1.  1.  1.  1.  0.  0.  1.  1.  0.  0.\n",
      "  1.  1.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  0.  0.  1.  0.  0.  0.\n",
      "  0.  1.]\n",
      "10 training accuracy 0.733085593194\n",
      "10 validation accuracy 0.726504914005\n",
      "[ 0.  1.  0.  1.  1.  1.  0.  1.  1.  1.  1.  1.  0.  0.  1.  0.  0.  0.\n",
      "  1.  1.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  0.  0.  1.  0.  0.  0.\n",
      "  0.  1.]\n",
      "20 training accuracy 0.763612911151\n",
      "20 validation accuracy 0.757371007371\n",
      "[ 0.  1.  0.  0.  1.  1.  0.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.\n",
      "  1.  1.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  0.  0.  1.  0.  0.  0.\n",
      "  0.  1.]\n",
      "30 training accuracy 0.801940972329\n",
      "30 validation accuracy 0.793611793612\n",
      "[ 0.  1.  0.  0.  1.  1.  0.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  0.  0.  1.  0.  0.  0.\n",
      "  0.  1.]\n",
      "40 training accuracy 0.827370166764\n",
      "40 validation accuracy 0.819563882064\n",
      "[ 0.  1.  0.  0.  1.  1.  0.  1.  1.  1.  1.  1.  0.  0.  1.  0.  0.  0.\n",
      "  1.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  0.  0.  1.  0.  0.  0.\n",
      "  0.  1.]\n",
      "50 training accuracy 0.765517029575\n",
      "50 validation accuracy 0.760595823096\n",
      "[ 0.  1.  0.  0.  1.  1.  0.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.\n",
      "  1.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  0.  0.  1.  0.  0.  0.\n",
      "  0.  1.]\n",
      "60 training accuracy 0.810478793649\n",
      "60 validation accuracy 0.805128992629\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  0.  0.  1.  0.  0.  0.\n",
      "  0.  1.]\n",
      "70 training accuracy 0.784128251589\n",
      "70 validation accuracy 0.785319410319\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  0.  0.  1.  0.  0.  0.\n",
      "  0.  1.]\n",
      "80 training accuracy 0.779828629342\n",
      "80 validation accuracy 0.780712530713\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  0.  0.  1.  0.  0.  0.\n",
      "  0.  1.]\n",
      "90 training accuracy 0.784404655877\n",
      "90 validation accuracy 0.78578009828\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  0.  0.  1.  0.  0.  0.\n",
      "  0.  1.]\n",
      "100 training accuracy 0.782930499678\n",
      "100 validation accuracy 0.783476658477\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  0.  0.  1.  0.  0.  0.\n",
      "  0.  1.]\n",
      "110 training accuracy 0.794170940696\n",
      "110 validation accuracy 0.795300982801\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  0.  0.  1.  0.  0.  0.\n",
      "  0.  1.]\n",
      "120 training accuracy 0.793679555296\n",
      "120 validation accuracy 0.794072481572\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  0.  0.  1.  0.  0.  0.\n",
      "  0.  1.]\n",
      "130 training accuracy 0.791959706397\n",
      "130 validation accuracy 0.791922604423\n",
      "[ 0.  0.  0.  0.  1.  1.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  0.  0.  1.  0.  0.  0.\n",
      "  0.  1.]\n",
      "140 training accuracy 0.807991155063\n",
      "140 validation accuracy 0.808353808354\n",
      "[ 0.  0.  0.  0.  1.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  0.  0.  1.  0.  0.  0.\n",
      "  0.  1.]\n",
      "150 training accuracy 0.798900525168\n",
      "150 validation accuracy 0.799600737101\n",
      "[ 0.  0.  0.  0.  1.  1.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  0.  0.  1.  0.  0.  0.\n",
      "  0.  1.]\n",
      "160 training accuracy 0.819262307669\n",
      "160 validation accuracy 0.81941031941\n",
      "[ 0.  0.  0.  0.  1.  1.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  0.  0.  1.  0.  0.  0.\n",
      "  0.  1.]\n",
      "170 training accuracy 0.803353705353\n",
      "170 validation accuracy 0.803286240786\n",
      "[ 0.  0.  0.  0.  1.  1.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  0.  0.  1.  0.  0.  0.\n",
      "  0.  1.]\n",
      "180 training accuracy 0.813857068272\n",
      "180 validation accuracy 0.813574938575\n",
      "[ 0.  0.  0.  0.  1.  1.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  0.  0.  1.  0.  0.  0.\n",
      "  0.  1.]\n",
      "190 training accuracy 0.809096772212\n",
      "190 validation accuracy 0.809121621622\n",
      "[ 0.  0.  0.  0.  1.  1.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  0.  0.  1.  0.  0.  0.\n",
      "  0.  1.]\n",
      "200 training accuracy 0.813580663985\n",
      "200 validation accuracy 0.813267813268\n",
      "[ 0.  0.  0.  0.  1.  1.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  0.  0.  1.  0.  0.  0.\n",
      "  0.  1.]\n",
      "210 training accuracy 0.811553699211\n",
      "210 validation accuracy 0.811425061425\n",
      "[ 0.  0.  0.  0.  1.  1.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  0.  0.  1.  0.  0.  0.\n",
      "  0.  1.]\n",
      "220 training accuracy 0.812843585885\n",
      "220 validation accuracy 0.812653562654\n",
      "[ 0.  0.  0.  0.  1.  1.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  0.  0.  1.  0.  0.  0.\n",
      "  0.  1.]\n",
      "230 training accuracy 0.81265931636\n",
      "230 validation accuracy 0.812653562654\n",
      "[ 0.  0.  0.  0.  1.  1.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  0.  0.  1.  0.  0.  0.\n",
      "  0.  1.]\n",
      "240 training accuracy 0.812935720647\n",
      "240 validation accuracy 0.812807125307\n",
      "[ 0.  0.  0.  0.  1.  1.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  0.  0.  1.  0.  0.  0.\n",
      "  0.  1.]\n",
      "250 training accuracy 0.81315070176\n",
      "250 validation accuracy 0.812807125307\n",
      "[ 0.  0.  0.  0.  1.  1.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  0.  0.  1.  0.  0.  0.\n",
      "  0.  1.]\n",
      "260 training accuracy 0.81339639446\n",
      "260 validation accuracy 0.813267813268\n",
      "[ 0.  0.  0.  0.  1.  1.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  0.  0.  1.  0.  0.  0.\n",
      "  0.  1.]\n",
      "270 training accuracy 0.813242836522\n",
      "270 validation accuracy 0.813114250614\n",
      "[ 0.  0.  0.  0.  1.  1.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  0.  0.  1.  0.  0.  0.\n",
      "  0.  1.]\n",
      "280 training accuracy 0.813058566997\n",
      "280 validation accuracy 0.812807125307\n",
      "[ 0.  0.  0.  0.  1.  1.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  0.  0.  1.  0.  0.  0.\n",
      "  0.  1.]\n",
      "290 training accuracy 0.813089278585\n",
      "290 validation accuracy 0.812960687961\n",
      "[ 0.  0.  0.  0.  1.  1.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  0.  0.  1.  0.  0.  0.\n",
      "  0.  1.]\n",
      "300 training accuracy 0.813887779859\n",
      "300 validation accuracy 0.814342751843\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-71d257910858>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0ml2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmy_lambda\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_batch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_batch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(arr, values, axis)\u001b[0m\n\u001b[1;32m   5145\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5146\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5147\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# optimizations to consider when hand-crafting everything: feature scaling (done), l1 l2 norm\n",
    "\n",
    "# grab validation set\n",
    "import random\n",
    "val_idxs = random.sample(range(0, len(X_train)), int(len(X_train) * 0.2))\n",
    "X_valid = X_train[val_idxs]\n",
    "Y_valid = Y_train[val_idxs]\n",
    "\n",
    "'''X_real_train = np.delete(X_train, val_idxs, 0)\n",
    "Y_real_train = np.delete(Y_train, val_idxs, 0)'''\n",
    "X_real_train = X_train\n",
    "Y_real_train = Y_train\n",
    "\n",
    "# old batch = 50 #len(X_real_train)\n",
    "batch = len(X_real_train)\n",
    "lr = 0.002\n",
    "# decay = 1\n",
    "my_lambda = 0 # l2 norm\n",
    "\n",
    "learning_rates = [1.0] # for adagrad\n",
    "\n",
    "\n",
    "# initialize weights\n",
    "weights = np.zeros(len(X_train[0])) # we already appended bias to X_train\n",
    "\n",
    "#===================saving previous weights======================\n",
    "# weights = weights_saved\n",
    "\n",
    "for epoch in range(0,5000):\n",
    "    # randomly decide training order in this epoch\n",
    "    # [41,6,0,231...]\n",
    "    random_idxs = random.sample(range(0, len(X_real_train)), len(X_real_train))\n",
    "    for i in range(0, len(random_idxs), batch):\n",
    "        X_batch = X_real_train[random_idxs[i: min(i+batch, len(random_idxs))]]\n",
    "        Y_batch = Y_real_train[random_idxs[i: min(i+batch, len(random_idxs))]]\n",
    "        Y_pred = np.dot(X_batch, weights)\n",
    "        Y_pred = sigmoid(Y_pred)\n",
    "        Y_pred = np.around(Y_pred)\n",
    "\n",
    "        # get gradients and update weights\n",
    "        gradient = np.zeros(len(X_train[0]))\n",
    "        for idx_batch in range(0, len(X_batch)):\n",
    "            l2 = 2 * my_lambda * np.append(weights[:-1],0)\n",
    "            gradient += - ((Y_batch[idx_batch] - Y_pred[idx_batch]) * X_batch[idx_batch] + l2) / batch\n",
    "\n",
    "        adagrad_denom = np.sqrt(sum(np.array(learning_rates) ** 2))\n",
    "        new_lr = lr / adagrad_denom\n",
    "        learning_rates += new_lr\n",
    "        # print(new_lr)\n",
    "        weights = weights - new_lr * gradient # * pow(decay, epoch)\n",
    "            \n",
    "    if epoch % 10 == 9:\n",
    "        train_pred = np.dot(X_real_train, weights)\n",
    "        train_pred = sigmoid(train_pred)\n",
    "        train_pred = np.around(train_pred)\n",
    "        print(train_pred[0:20])\n",
    "        print(Y_real_train[0:20])\n",
    "        print(epoch+1, 'training accuracy', np.sum(train_pred == Y_real_train) / (len(Y_real_train)))\n",
    "\n",
    "        valid_pred = np.dot(X_valid, weights)\n",
    "        valid_pred = sigmoid(valid_pred)\n",
    "        valid_pred = np.around(valid_pred)\n",
    "        print(epoch+1, 'validation accuracy', np.sum(valid_pred == Y_valid) / (len(Y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"weights_saved = weights\\nprint(weights_saved)\\nnp.save('weights_logistic', weights_saved)\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0.82\n",
    "'''weights_saved = weights\n",
    "print(weights_saved)\n",
    "np.save('weights_logistic', weights_saved)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.\n",
      "  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "X_test = []\n",
    "\n",
    "with open('data/X_test.csv', 'rt', encoding='big5') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "\n",
    "    row1 = next(reader) # skip headings\n",
    "    for row in reader:\n",
    "        X_test.append([float(i) for i in row] + [1.0])\n",
    "            \n",
    "X_test = np.array(X_test)\n",
    "for col in [0,1,3,4,5]:\n",
    "    if np.std(X_test[:,col]) != 0:\n",
    "        X_test[:,col] = np.divide((X_test[:,col] - np.average(X_test[:,col])), np.std(X_test[:,col]))\n",
    "\n",
    "pred = np.dot(X_test, weights)\n",
    "pred = sigmoid(pred)\n",
    "pred = np.around(pred)\n",
    "print(pred[0:100])\n",
    "\n",
    "with open('data/submission.csv', 'wt') as outfile:\n",
    "    test_writer = csv.writer(outfile)\n",
    "    test_writer.writerow(['id','label'])\n",
    "    \n",
    "    counter = 0\n",
    "    for num in pred:\n",
    "        counter += 1\n",
    "        test_writer.writerow([str(counter),int(num)])\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
